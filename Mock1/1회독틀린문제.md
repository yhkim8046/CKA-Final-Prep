## 1회독 틀린문제

### Q1:
Create a Pod mc-pod in the mc-namespace namespace with three containers. The first container should be named mc-pod-1, run the nginx:1-alpine image, and set an environment variable NODE_NAME to the node name. The second container should be named mc-pod-2, run the busybox:1 image, and continuously log the output of the date command to the file /var/log/shared/date.log every second. The third container should have the name mc-pod-3, run the image busybox:1, and print the contents of the date.log file generated by the second container to stdout. Use a shared, non-persistent volume.

### answer:
```
apiVersion: v1
kind: Pod
metadata:
  name: mc-pod
  namespace: mc-namespace
spec:
  containers:
    - name: mc-pod-1
      image: nginx:1-alpine
      env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
    - name: mc-pod-2
      image: busybox:1
      volumeMounts:
        - name: shared-volume
          mountPath: /var/log/shared
      command:
        - "sh"
        - "-c"
        - "while true; do date >> /var/log/shared/date.log; sleep 1; done"
    - name: mc-pod-3
      image: busybox:1
      command:
        - "sh"
        - "-c"
        - "tail -f /var/log/shared/date.log"
      volumeMounts:
        - name: shared-volume
          mountPath: /var/log/shared
  volumes:
    - name: shared-volume
      emptyDir: {}
```
---

시크릿이나 환경변수를 볼륨으로 마운트하여 경로가 항상 달라지지 않게 사용할 수 있음
볼륨과 볼륨 마운트의 이름은 동일해야함




---
### Q2:
This question needs to be solved on node node01. To access the node using SSH, use the credentials below:

username: bob
password: caleston123

As an administrator, you need to prepare node01 to install kubernetes. One of the steps is installing a container runtime. Install the cri-docker_0.3.16.3-0.debian.deb package located in /root and ensure that the cri-docker service is running and enabled to start on boot.

### answer:
Use dpkg to install the package and systemctl to manage the service.

SSH to node01 as follows, using password caleston123:

```
ssh bob@node01
```

Switch to root using sudo -i or prefix the commands below using sudo.

The cri-docker_0.3.16.3-0.debian.deb package is located in the /root directory. Use dpkg to install the package:
```
dpkg -i /root/cri-docker_0.3.16.3-0.debian.deb
```
After installing the package, start the cri-docker service and enable it to start on boot:
```
systemctl start cri-docker
systemctl enable cri-docker
```

Verify that the cri-docker service is running:

```
systemctl is-active cri-docker
```

Check that it is enabled to start on boot:

```
systemctl is-enabled cri-docker
```

You should see active and enabled as the output for both commands

### Q3 
On controlplane node, identify all CRDs related to VerticalPodAutoscaler and save their names into the file /root/vpa-crds.txt.

### answer:
Use kubectl get crd to find all CRDs and filter by VerticalPodAutoscaler.
Remember to exit back to controlplane node before attempting this question.
To find the CRDs Related to VerticalPodAutoscaler, use the following kubectl command to list all CRDs and filter them for VerticalPodAutoscaler resources:

```
kubectl get crd -o custom-columns=NAME:.metadata.name | grep verticalpodautoscaler > /root/vpa-crds.txt
```

Ensure that the CRD names are correctly saved in the /root/vpa-crds.txt
The file should contain the CRD names related to VerticalPodAutoscaler.

### Q4
Create a service messaging-service to expose the messaging application within the cluster on port 6379.
Use imperative commands.

### answer:
```
kubectl expose pod messaging --port=6379 --name messaging-service
```

### Q7
Expose the hr-web-app created in the previous task as a service named hr-web-app-service, accessible on port 30082 on the nodes of the cluster.
The web application listens on port 8080.

### answer:
Run the command: 
```
kubectl expose deployment hr-web-app --type=NodePort --port=8080 --name=hr-web-app-service --dry-run=client -o yaml > hr-web-app-service.yaml 
```
to generate a service definition file.
Now, in generated service definition file add the nodePort field with the given port number under the ports section and create a service.

### Q8
Create a Horizontal Pod Autoscaler (HPA) with name webapp-hpa for the deployment named kkapp-deploy in the default namespace with the webapp-hpa.yaml file located under the root folder.
Ensure that the HPA scales the deployment based on CPU utilization, maintaining an average CPU usage of 50% across all pods.
Configure the HPA to cautiously scale down pods by setting a stabilization window of 300 seconds to prevent rapid fluctuations in pod count.

Note: The kkapp-deploy deployment is created for backend; you can check in the terminal.

### answer:
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kkapp-deploy
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
```
```
kubectl create -f webapp-hpa.yaml
```


### Q10: 
Deploy a Vertical Pod Autoscaler (VPA) with name analytics-vpa for the deployment named analytics-deployment in the default namespace.
The VPA should automatically adjust the CPU and memory requests of the pods to optimize resource utilization. Ensure that the VPA operates in Auto mode, allowing it to evict and recreate pods with updated resource requests as needed.

### Answer:

```
kubectl create -n default -f - <<EOF
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: analytics-deployment
  updatePolicy:
    updateMode: "Auto"
EOF
```

### Q11: 
Create a Kubernetes Gateway resource with the following specifications:
```
Name: web-gateway
Namespace: nginx-gateway
Gateway Class Name: nginx
Listeners:
    Protocol: HTTP
    Port: 80
    Name: http
```

### Answer:

```
kubectl create -n nginx-gateway -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      protocol: HTTP
      port: 80
EOF
```

### Q12:
One co-worker deployed an nginx helm chart kk-mock1 in the kk-ns namespace on the cluster. A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.


After updating the helm chart, upgrade the helm chart version to 18.1.15.
In this task, we will use the kubectl and helm commands. Here are the steps: -


### answer:
use the helm ls command to list all the releases installed using Helm in the Kubernetes cluster.

```
helm ls -A
```

Here -A or --all-namespaces option lists all the releases of all the namespaces.
Identify the namespace where the resources get deployed.
Use the helm repo ls command to list the helm repositories.

```
helm repo ls 
```
Now, update the helm repository with the following command: -
```
helm repo update kk-mock1 -n kk-ns
```

The above command updates the local cache of available charts from the configured chart repositories.
The helm search command searches for all the available charts in a specific Helm chart repository. In our case, it's the nginx helm chart.
```
helm search repo kk-mock1/nginx -n kk-ns -l | head -n30
```

The -l or --versions option is used to display information about all available chart versions.

Upgrade the helm chart to 18.1.15 and also, increase the replica count of the deployment to 2 from the command line. Use the helm upgrade command as follows: -

```
helm upgrade kk-mock1 kk-mock1/nginx -n kk-ns --version=18.1.15 
```

After upgrading the chart version, you can verify it with the following command: -

```
helm ls -n kk-ns
```

Look under the CHART column for the chart version.
